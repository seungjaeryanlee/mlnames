L1 regularization, see Lasso
Activation function, 392–395
AdaBoost, 337–346
Adaptive lasso, 92
Adaptive methods, 429
Adaptive nearest neighbor methods, 475–478
Adaptive wavelet filtering, 181
Additive model, 295–304
Adjusted response, 297
Affine set, 130
Affine-invariant average, 482, 540
AIC, see Akaike information criterion
Akaike information criterion (AIC),
230
Analysis of deviance, 124
Applications
abstracts, 672
aorta, 204
bone, 152
California housing, 371–372,
591
countries, 517
demographics, 379–380
document, 532
flow cytometry, 637
galaxy, 201
heart attack, 122, 146, 207
lymphoma, 674
marketing, 488
microarray, 5, 505, 532
nested spheres, 590
New Zealand fish, 375–379
nuclear magnetic resonance,
176
ozone, 201
prostate cancer, 3, 49, 61, 608
protein mass spectrometry, 664
satellite image, 470
skin of the orange, 429–432
spam, 2, 300–304, 313, 320,
328, 352, 593
vowel, 440, 464
waveform, 451
ZIP code, 4, 404, 536–539
Archetypal analysis, 554–557
Association rules, 492–495, 499–
501
Automatic relevance determination,
411
Automatic selection of smoothing
parameters , 156
B-Spline, 186
Back-propagation, 392–397, 408–
409
Backfitting, 297, 391
Backward
selection, 58
stepwise selection, 59
Backward pass, 396
Bagging, 282–288, 409, 587
Basis expansions and regularization, 139–189
Basis functions, 141, 186, 189, 321,
328
Batch learning, 397
Baum–Welch algorithm, 272
Bayes
classifier, 21
factor, 234
methods, 233–235, 267–272
rate, 21
Bayesian, 409
Bayesian information criterion (BIC),
233
Benjamini–Hochberg method, 688
Best-subset selection, 57, 610
Between class covariance matrix,
114
Bias, 16, 24, 37, 160, 219
Bias-variance decomposition, 24,
37, 219
Bias-variance tradeoff, 37, 219
BIC, see Bayesian Information Criterion
Boltzmann machines, 638–648
Bonferroni method, 686
Boosting, 337–386, 409
as lasso regression, 607–609
exponential loss and AdaBoost,
343
gradient boosting, 358
implementations, 360
margin maximization, 613
numerical optimization, 358
partial-dependence plots, 369
regularization path, 607
shrinkage, 364
stochastic gradient boosting,
365
tree size, 361
variable importance, 367
Bootstrap, 249, 261–264, 267, 271–
282, 587
relationship to Bayesian method,
271
relationship to maximum likelihood method, 267
Bottom-up clustering, 520–528
Bump hunting, see Patient rule
induction method
Bumping, 290–292
C5.0, 624
Canonical variates, 441
CART, see Classification and regression trees
Categorical predictors, 10, 310
Censored data, 674
Classical multidimensional scaling,
570
Classification, 22, 101–137, 305–
317, 417–429
Classification and regression trees
(CART), 305–317
Clique, 628
Clustering, 501–528
k-means, 509–510
agglomerative, 523–528
hierarchical, 520–528
Codebook, 515
Combinatorial algorithms, 507
Combining models, 288–290
Committee, 289, 587, 605
Comparison of learning methods,
350–352
Complete data, 276
Complexity parameter, 37
Computational shortcuts
quadratic penalty, 659
Condensing procedure, 480
Conditional likelihood, 31
Confusion matrix, 301
Conjugate gradients, 396
Consensus, 285–286
Convolutional networks, 407
Coordinate descent, 92, 636, 668
COSSO, 304
Cost complexity pruning, 308
Covariance graph, 631
Cp statistic, 230
Cross-entropy, 308–310
Cross-validation, 241–245
Cubic smoothing spline, 151–153
Cubic spline, 151–153
Curse of dimensionality, 22–26
Dantzig selector, 89
Data augmentation, 276
Daubechies symmlet-8 wavelets,
176
De-correlation, 597
Decision boundary, 13–15, 21
Decision trees, 305–317
Decoder, 515, see encoder
Decomposable models, 641
Degrees of freedom
in an additive model, 302
in ridge regression, 68
of a tree, 336
of smoother matrices, 153–154,
158
Delta rule, 397
Demmler-Reinsch basis for splines,
156
Density estimation, 208–215
Deviance, 124, 309
Diagonal linear discriminant analysis, 651–654
Dimension reduction, 658
for nearest neighbors, 479
Discrete variables, 10, 310–311
Discriminant
adaptive nearest neighbor classifier, 475–480
analysis, 106–119
coordinates, 108
functions, 109–110
Dissimilarity measure, 503–504
Dummy variables, 10
Early stopping, 398
Effective degrees of freedom, 17,
68, 153–154, 158, 232, 302,
336
Effective number of parameters,
15, 68, 153–154, 158, 232,
302, 336
Eigenvalues of a smoother matrix,
154
Elastic net, 662
EM algorithm, 272–279
as a maximization-maximization
procedure, 277
for two component Gaussian
mixture, 272
Encoder, 514–515
Ensemble, 616–623
Ensemble learning, 605–624
Entropy, 309
Equivalent kernel, 156
Error rate, 219–230
Error-correcting codes, 606
Estimates of in-sample prediction
error, 230
Expectation-maximization algorithm,
see EM algorithm
Extra-sample error, 228
False discovery rate, 687–690, 692,
693
Feature, 1
extraction, 150
selection, 409, 658, 681–683
Feed-forward neural networks, 392–
408
Fisher’s linear discriminant, 106–
119, 438
Flexible discriminant analysis, 440–
445
Forward
selection, 58
stagewise, 86, 608
stagewise additive modeling,
342
stepwise, 73
Forward pass algorithm, 395
Fourier transform, 168
Frequentist methods, 267
Function approximation, 28–36
Fused lasso, 666
Gap statistic, 519
Gating networks, 329
Gauss-Markov theorem, 51–52
Gauss-Newton method, 391
Gaussian (normal) distribution, 16
Gaussian graphical model, 630
Gaussian mixtures, 273, 463, 492,
509
Gaussian radial basis functions,
212
GBM, see Gradient boosting
GBM package, see Gradient boosting
GCV, see Generalized cross-validation
GEM (generalized EM), 277
Generalization
error, 220
performance, 220
Generalized additive model, 295–
304
Generalized association rules, 497–
499
Generalized cross-validation, 244
Generalized linear discriminant analysis, 438
Generalized linear models, 125
Gibbs sampler, 279–280, 641
for mixtures, 280
Gini index, 309
Global Markov property, 628
Gradient Boosting, 359–361
Gradient descent, 358, 395–397
Graph Laplacian, 545
Graphical lasso, 636
Grouped lasso, 90
Haar basis function, 176
Hammersley-Clifford theorem, 629
Hard-thresholding, 653
Hat matrix, 46
Helix, 582
Hessian matrix, 121
Hidden nodes, 641–642
Hidden units, 393–394
Hierarchical clustering, 520–528
Hierarchical mixtures of experts,
329–332
High-dimensional problems, 649
Hints, 96
Hyperplane, see Separating Hyperplane
ICA, see Independent components
analysis
Importance sampling, 617
In-sample prediction error, 230
Incomplete data, 332
Independent components analysis,
557–570
Independent variables, 9
Indicator response matrix, 103
Inference, 261–294
Information
Fisher, 266
observed, 274
Information theory, 236, 561
Inner product, 53, 668, 670
Inputs, 10
Instability of trees, 312
Intercept, 11
Invariance manifold, 471
Invariant metric, 471
Inverse wavelet transform, 179
IRLS, see Iteratively reweighted
least squares
Irreducible error, 224
Ising model, 638
ISOMAP, 572
Isometric feature mapping, 572
Iterative proportional scaling, 585
Iteratively reweighted least squares
(IRLS), 121
Jensen’s inequality, 293
Join tree, 629
Junction tree, 629
K-means clustering, 460, 509–514
K-medoid clustering, 515–520
K-nearest neighbor classifiers, 463
Karhunen-Loeve transformation (principal components), 66–
67, 79, 534–539
Karush-Kuhn-Tucker conditions,
133, 420
Kernel
classification, 670
density classification, 210
density estimation, 208–215
function, 209
logistic regression, 654
principal component, 547–550
string, 668–669
trick, 660
Kernel methods, 167–176, 208–215,
423–438, 659
Knot, 141, 322
Kriging, 171
Kruskal-Shephard scaling, 570
Kullback-Leibler distance, 561
Lagrange multipliers, 293
Landmark, 539
Laplacian, 545
Laplacian distribution, 72
LAR, see Least angle regression
Lasso, 68–69, 86–90, 609, 635, 636,
661
fused, 666
Latent
factor, 674
variable, 678
Learning, 1
Learning rate, 396
Learning vector quantization, 462
Least angle regression, 73–79, 86,
610
Least squares, 11, 32
Leave-one-out cross-validation, 243
LeNet, 406
Likelihood function, 265, 273
Linear basis expansion, 139–148
Linear combination splits, 312
Linear discriminant function, 106–
119
Linear methods
for classification, 101–137
for regression, 43–99
Linear models and least squares,
11
Linear regression of an indicator
matrix, 103
Linear separability, 129
Linear smoother, 153
Link function, 296
LLE, see Local linear embedding
Local false discovery rate, 693
Local likelihood, 205
Local linear embedding, 572
Local methods in high dimensions,
22–27
Local minima, 400
Local polynomial regression, 197
Local regression, 194, 200
Localization in time/frequency, 175
Loess (local regression), 194, 200
Log-linear model, 639
Log-odds ratio (logit), 119
Logistic (sigmoid) function, 393
Logistic regression, 119–128, 299
Logit (log-odds ratio), 119
Loss function, 18, 21, 219–223, 346
Loss matrix, 310
Lossless compression, 515
Lossy compression, 515
LVQ, see Learning Vector Quantization
Mahalanobis distance, 441
Majority vote, 337
Majorization, 294, 553
Majorize-Minimize algorithm, 294,
584
MAP (maximum aposteriori) estimate, 270
Margin, 134, 418
Market basket analysis, 488, 499
Markov chain Monte Carlo (MCMC)
methods, 279
Markov graph, 627
Markov networks, 638–648
MARS, see Multivariate adaptive
regression splines
MART, see Multiple additive regression trees
Maximum likelihood estimation,
31, 261, 265
MCMC, see Markov Chain Monte
Carlo Methods
MDL, see Minimum description
length
Mean field approximation, 641
Mean squared error, 24, 285
Memory-based method, 463
Metropolis-Hastings algorithm, 282
Minimum description length (MDL),
235
Minorization, 294, 553
Minorize-Maximize algorithm, 294,
584
Misclassification error, 17, 309
Missing data, 276, 332–333
Missing predictor values, 332–333
Mixing proportions, 214
Mixture discriminant analysis, 449–
455
Mixture modeling, 214–215, 272–
275, 449–455, 692
Mixture of experts, 329–332
Mixtures and the EM algorithm,
272–275
MM algorithm, 294, 584
Mode seekers, 507
Model averaging and stacking, 288
Model combination, 289
Model complexity, 221–222
Model selection, 57, 222–223, 230–
231
Modified regression, 634
Monte Carlo method, 250, 495
Mother wavelet, 178
Multidimensional scaling, 570–572
Multidimensional splines, 162
Multiedit algorithm, 480
Multilayer perceptron, 400, 401
Multinomial distribution, 120
Multiple additive regression trees
(MART), 361
Multiple hypothesis testing, 683–
693
Multiple minima, 291, 400
Multiple outcome shrinkage and
selection, 84
Multiple outputs, 56, 84, 103–106
Multiple regression from simple univariate regression, 52
Multiresolution analysis, 178
Multivariate adaptive regression
splines (MARS), 321–327
Multivariate nonparametric regression, 445
Nadaraya–Watson estimate, 193
Naive Bayes classifier, 108, 210–
211, 694
Natural cubic splines, 144–146
Nearest centroids, 670
Nearest neighbor methods, 463–
483
Nearest shrunken centroids, 651–
654, 694
Network diagram, 392
Neural networks, 389–416
Newton’s method (Newton-Raphson
procedure), 120–122
Non-negative matrix factorization,
553–554
Nonparametric logistic regression,
299–304
Normal (Gaussian) distribution,
16, 31
Normal equations, 12
Numerical optimization, 395–396
Object dissimilarity, 505–507
Online algorithm, 397
Optimal scoring, 445, 450–451
Optimal separating hyperplane, 132–
135
Optimism of the training error rate,
228–230
Ordered categorical (ordinal) predictor, 10, 504
Ordered features, 666
Orthogonal predictors, 53
Overfitting, 220, 228–230, 364
PageRank, 576
Pairwise distance, 668
Pairwise Markov property, 628
Parametric bootstrap, 264
Partial dependence plots, 369–370
Partial least squares, 80–82, 680
Partition function, 638
Parzen window, 208
Pasting, 318
Path algorithm, 73–79, 86–89, 432
Patient rule induction method(PRIM),
317–321, 499–501
Peeling, 318
Penalization, 607, see regularization
Penalized discriminant analysis, 446–
449
Penalized polynomial regression,
171
Penalized regression, 34, 61–69, 171
Penalty matrix, 152, 189
Perceptron, 392–416
Piecewise polynomials and splines,
36, 143
Posterior
distribution, 268
probability, 233–235, 268
Power method, 577
Pre-conditioning, 681–683
Prediction accuracy, 329
Prediction error, 18
Predictive distribution, 268
PRIM, see Patient rule induction
method
Principal components, 66–67, 79–
80, 534–539, 547
regression, 79–80
sparse, 550
supervised, 674
Principal curves and surfaces, 541–
544
Principal points, 541
Prior distribution, 268–272
Procrustes
average, 540
distance, 539
Projection pursuit, 389–392, 565
regression, 389–392
Prototype classifier, 459–463
Prototype methods, 459–463
Proximity matrices, 503
Pruning, 308
QR decomposition, 55
Quadratic approximations and inference, 124
Quadratic discriminant function,
108, 110
Radial basis function (RBF) network, 392
Radial basis functions, 212–214,
275, 393
Radial kernel, 548
Random forest, 409, 587–604
algorithm, 588
bias, 596–601
comparison to boosting, 589
example, 589
out-of-bag (oob), 592
overfit, 596
proximity plot, 595
variable importance, 593
variance, 597–601
Rao score test, 125
Rayleigh quotient, 116
Receiver operating characteristic
(ROC) curve, 317
Reduced-rank linear discriminant
analysis, 113
Regression, 11–14, 43–99, 200–204
Regression spline, 144
Regularization, 34, 167–176
Regularized discriminant analysis,
112–113, 654
Relevance network, 631
Representer of evaluation, 169
Reproducing kernel Hilbert space,
167–176, 428–429
Reproducing property, 169
Responsibilities, 274–275
Ridge regression, 61–68, 650, 659
Risk factor, 122
Robust fitting, 346–350
Rosenblatt’s perceptron learning
algorithm, 130
Rug plot, 303
Rulefit, 623
SAM, 690–693, see Significance Analysis of Microarrays
Sammon mapping, 571
SCAD, 92
Scaling of the inputs, 398
Schwarz’s criterion, 230–235
Score equations, 120, 265
Self-consistency property, 541–543
Self-organizing map (SOM), 528–
534
Sensitivity of a test, 314–317
Separating hyperplane, 132–135
Separating hyperplanes, 136, 417–
419
Separator, 628
Shape average, 482, 540
Shrinkage methods, 61–69, 652
Sigmoid, 393
Significance Analysis of Microarrays, 690–693
Similarity measure, see Dissimilarity measure
Single index model, 390
Singular value decomposition, 64,
535–536, 659
singular values, 535
singular vectors, 535
Sliced inverse regression, 480
Smoother, 139–156, 192–199
matrix, 153
Smoothing parameter, 37, 156–161,
198–199
Smoothing spline, 151–156
Soft clustering, 512
Soft-thresholding, 653
Softmax function, 393
SOM, see Self-organizing map
Sparse, 175, 304, 610–613, 636
additive model, 91
graph, 625, 635
Specificity of a test, 314–317
Spectral clustering, 544–547
Spline, 186
additive, 297–299
cubic, 151–153
cubic smoothing, 151–153
interaction, 428
regression, 144
smoothing, 151–156
thin plate, 165
Squared error loss, 18, 24, 37, 219
SRM, see Structural risk minimization
Stacking (stacked generalization),
290
Starting values, 397
Statistical decision theory, 18–22
Statistical model, 28–29
Steepest descent, 358, 395–397
Stepwise selection, 60
Stochastic approximation, 397
Stochastic search (bumping), 290–
292
Stress function, 570–572
Structural risk minimization (SRM),
239–241
Subset selection, 57–60
Supervised learning, 2
Supervised principal components,
674–681
Support vector classifier, 417–421,
654
multiclass, 657
Support vector machine, 423–437
SURE shrinkage method, 179
Survival analysis, 674
Survival curve, 674
SVD, see Singular value decomposition
Symmlet basis, 176
Tangent distance, 471–475
Tanh activation function, 424
Target variables, 10
Tensor product basis, 162
Test error, 220–223
Test set, 220
Thin plate spline, 165
Thinning strategy, 189
Trace of a matrix, 153
Training epoch, 397
Training error, 220–223
Training set, 219–223
Tree for regression, 307–308
Tree-based methods, 305–317
Trees for classification, 308–310
Trellis display, 202
Undirected graph, 625–648
Universal approximator, 390
Unsupervised learning, 2, 485–585
Unsupervised learning as supervised learning, 495–497
Validation set, 222
Vapnik-Chervonenkis (VC) dimension, 237–239
Variable importance plot, 594
Variable types and terminology, 9
Variance, 16, 25, 37, 158–161, 219
between, 114
within, 114, 446
Variance reduction, 588
Varying coefficient models, 203–
204
VC dimension, see Vapnik–Chervonenkis dimension
Vector quantization, 514–515
Voronoi regions, 510
Wald test, 125
Wavelet
basis functions, 176–179
smoothing, 174
transform, 176–179
Weak learner, 383, 605
Weakest link pruning, 308
Webpages, 576
Website for book, 8
Weight decay, 398
Weight elimination, 398
Weights in a neural network, 395
Within class covariance matrix, 114,
446